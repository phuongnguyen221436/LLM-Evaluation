### LLM Evaluation Hub

# ğŸ§  LLM Evaluation Hub

An open-source tool to systematically evaluate prompts, compare LLM performance, and track cost, latency, and accuracy â€” all in one place.

## ğŸš€ Why?
Prompt engineering is still trial-and-error. This hub lets you run tests like a developer â€” with test cases, metrics, and comparisons across models and versions.

## ğŸ”§ Features
- ğŸ” **Prompt Playground**: Compare GPT-4, Claude, etc.
- ğŸ§ª **Test Case Evaluator**: Run JSON/CSV prompt+expected pairs
- ğŸ“Š **Metrics Dashboard**: Accuracy, latency, cost per run
- ğŸ” **Async Job Runner**: Process large test sets smoothly
- ğŸ§  **Feedback Loop** (WIP): Score model quality over time

## ğŸ›  Tech Stack
- **Backend**: FastAPI + PostgreSQL
- **Async Tasks**: Celery + Redis
- **LLM API**: OpenAI (GPT-4), Anthropic Claude
- **Frontend**: (Optional) React / Streamlit / CLI
- **Deployment**: Docker + Render/Fly.io

## ğŸ—‚ Project Structure
```
llm-eval-hub/
â”œâ”€â”€ backend/             # FastAPI backend
â”‚   â”œâ”€â”€ main.py          # API entrypoint
â”‚   â”œâ”€â”€ routes/          # API routes
â”‚   â”œâ”€â”€ services/        # Business logic
â”‚   â”œâ”€â”€ models/          # SQLAlchemy + Pydantic
â”‚   â””â”€â”€ utils/           # Helpers
â”œâ”€â”€ worker/              # Celery task queue
â”œâ”€â”€ frontend/            # Optional UI
â”œâ”€â”€ tests/               # Unit tests
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ“¦ Setup
```bash
git clone https://github.com/yourname/llm-eval-hub.git
cd llm-eval-hub
cp .env.example .env
# Fill in OpenAI keys, DB URL, etc.
docker-compose up --build
```

## ğŸ“¤ Sample Input Format
```json
[
  {"input": "Summarize: Apple released new iPhones...", "expected": "Apple announced new iPhones"},
  {"input": "Extract date: Our meeting is on April 5.", "expected": "April 5"}
]
```

## ğŸ“ˆ Roadmap
- [x] Prompt Playground API
- [x] Batch Evaluation Runner
- [x] PostgreSQL result storage
- [ ] Vector search for prior prompts
- [ ] Slack or CLI interface
- [ ] Model scoring w/ feedback loop

## ğŸ“„ License
MIT

---
_ğŸ‘‹ Built by Brielle Nguyen to make prompt testing less painful and more reliable._
